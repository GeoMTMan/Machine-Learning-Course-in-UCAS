<a name="content">目录</a>

[西瓜书笔记](#title)
- [1. 绪论](#1-introduction)
- [2. 线性模型](#2-linear-model)
	- [2.1. 线性回归](#2-linear-regression)
	- [2.2. logistic回归](#2-logistic-regression)
	- [2.3. 线性判别分析](#2-lda)
	- [2.4. 多分类学习](#2-multi-classfication-task)
		- [2.4.1. 一对一(OvO)](#2-one-vs-one)
		- [2.4.2. 一对其余(OvR)](#2-one-vs-rest)
		- [2.4.3. 多对多(MvM)](#2-many-vs-many)
- [3. 决策树](#3-decision-tree)
	- [3.1. 基本流程](#3-basic-steps)
	- [3.2. 划分选择](#3-partation-feature-chose)
		- [3.2.1. 信息熵 (information entropy)](#3-information-entropy)
		- [3.2.2. 信息增益 (information gain)](#3-information-gain)
		- [3.2.3. 增益率](#3-gain-ratio)
		- [3.2.4. 基尼指数](#3-gini-index)
	- [3.3. 剪枝处理](#3-pruning)
		- [3.3.1. 预剪枝](#3-prepruning)
		- [3.3.2. 后剪枝](#3-post-pruning)
	- [3.4. 连续与缺失值](#3-continuous-and-missing-value)
		- [3.4.1. 连续值处理](#3-continuous-value)
		- [3.4.2. 缺失值处理](#3-missing-value)
			- [3.4.2.1. 问题(1)划分属性选择](#3-issue1-for-missing-value)
			- [3.4.2.2. 问题(2)属性值缺失样本的划分](#3-issue2-for-missing-value)
- [番外一：一小时打完一次比赛](#extern1-one-hour-a-competation)
- [番外二：sklearn包中logistic回归算法的使用](#extern2-use-logistic-regression-in-sklearn)
	- [*2.1. 与logistic回归相关的类](#extern2-relative-class)
	- [*2.2. 优化算法](#extern2-optimize-algorithmn)
	- [*2.3. 正则化参数的选择：penalty](#extern2-regeneralization-parameter)
	- [*2.4. 分类方式选择参数：multi-class](#extern2-classify-method)



<h1 name="title">西瓜书笔记</h1>

<a name="1-introduction"><h2>1. 绪论 [<sup>目录</sup>](#content)</h2></a>

<p align="center"><img src=./picture/MachineLearning-ZZH-01.jpg width=800 /></p>

<p align="center"><img src=./picture/MachineLearning-ZZH-02.jpg width=800 /></p>

<p align="center"><img src=./picture/MachineLearning-ZZH-03.jpg width=800 /></p>

<p align="center"><img src=./picture/MachineLearning-ZZH-04.jpg width=800 /></p>

<a name="2-linear-model"><h2>2. 线性模型 [<sup>目录</sup>](#content)</h2></a>

<a name="2-linear-regression"><h3>2.1. 线性回归 [<sup>目录</sup>](#content)</h3></a>

<p align="center"><img src=./picture/MachineLearning-ZZH-08.jpg width=800 /></p>

<p align="center"><img src=./picture/MachineLearning-ZZH-09_1.jpg width=800 /></p>

<a name="2-logistic-regression"><h3>2.2. logistic回归 [<sup>目录</sup>](#content)</h3></a>

<p align="center"><img src=./picture/MachineLearning-ZZH-09_2.jpg width=800 /></p>

<p align="center"><img src=./picture/MachineLearning-ZZH-10.jpg width=800 /></p>

<p align="center"><img src=./picture/MachineLearning-ZZH-11.jpg width=800 /></p>

<p align="center"><img src=./picture/MachineLearning-ZZH-12_1.jpg width=800 /></p>

<a name="2-lda"><h3>2.3. 线性判别分析 [<sup>目录</sup>](#content)</h3></a>

<p align="center"><img src=./picture/MachineLearning-ZZH-12_2.jpg width=800 /></p>

<p align="center"><img src=./picture/MachineLearning-ZZH-13.jpg width=800 /></p>

<p align="center"><img src=./picture/MachineLearning-ZZH-14.jpg width=800 /></p>

<p align="center"><img src=./picture/MachineLearning-ZZH-15.jpg width=800 /></p>

<p align="center"><img src=./picture/MachineLearning-ZZH-16_1.jpg width=800 /></p>

<a name="2-multi-classfication-task"><h3>2.4. 多分类学习 [<sup>目录</sup>](#content)</h3></a>

<p align="center"><img src=./picture/MachineLearning-ZZH-16_2.jpg width=800 /></p>

<p align="center"><img src=./picture/MachineLearning-ZZH-17_1.jpg width=800 /></p>

<a name="2-one-vs-one"><h4>2.4.1. 一对一(OvO) [<sup>目录</sup>](#content)</h4></a>

<p align="center"><img src=./picture/MachineLearning-ZZH-17_2.jpg width=800 /></p>

<a name="2-one-vs-rest"><h4>2.4.2. 一对其余(OvR) [<sup>目录</sup>](#content)</h4></a>

<p align="center"><img src=./picture/MachineLearning-ZZH-17_3.jpg width=800 /></p>

<p align="center"><img src=./picture/MachineLearning-ZZH-18_1.jpg width=800 /></p>

<a name="2-many-vs-many"><h4>2.4.3. 多对多(MvM) [<sup>目录</sup>](#content)</h4></a>

<p align="center"><img src=./picture/MachineLearning-ZZH-18_2.jpg width=800 /></p>

<p align="center"><img src=./picture/MachineLearning-ZZH-19.jpg width=800 /></p>

<a name="3-decision-tree"><h2>3. 决策树 [<sup>目录</sup>](#content)</h2></a>

<p align="center"><img src=./picture/MachineLearning-ZZH-2
4.jpg width=800 /></p>

<a name="3-basic-steps"><h3>3.1. 基本流程 [<sup>目录</sup>](#content)</h3></a>

<p align="center"><img src=./picture/MachineLearning-ZZH-25.jpg width=800 /></p>

<p align="center"><img src=./picture/MachineLearning-ZZH-26_1.jpg width=800 /></p>

<a name="3-partation-feature-chose"><h3>3.2. 划分选择 [<sup>目录</sup>](#content)</h3></a>

<p align="center"><img src=./picture/MachineLearning-ZZH-26_2.jpg width=800 /></p>

<a name="3-information-entropy"><h4>3.2.1. 信息熵 (information entropy) [<sup>目录</sup>](#content)</h4></a>

<p align="center"><img src=./picture/MachineLearning-ZZH-26_3.jpg width=800 /></p>

Ent(D)的值越小，则D的纯度越高

<a name="3-information-gain"><h4>3.2.2. 信息增益 (information gain) [<sup>目录</sup>](#content)</h4></a>

<p align="center"><img src=./picture/MachineLearning-ZZH-27.jpg width=800 /></p>

ID3算法的缺点：

> - (1) ID3没有考虑连续特征，比如长度，密度都是连续值，无法在ID3运用。这大大限制了ID3的用途
> 
> - (2) ID3采用信息增益大的特征优先建立决策树的节点。很快就被人发现，在相同条件下，取值比较多的特征比取值少的特征信息增益大。比如一个变量有2个值，各为1/2，另一个变量为3个值，各为1/3，其实他们都是完全不确定的变量，但是取3个值的比取2个值的信息增益大
> 
> - (3) ID3算法对于缺失值的情况没有做考虑
> 
> - (4) 没有考虑过拟合的问题

针对各个问题，发展出来了对应的处理策略：

> - 对应(1)，[连续值的处理](#3-continuous-value)：使用连续属性离散化技术——二分法
> 
> - 对应(2)，[增益率](#3-gain-ratio)
> 
> - 对应(3)，[缺失值处理](#3-missing-value)
> 
> - 对应(4)，[剪枝处理](#3-pruning)

<a name="3-gain-ratio"><h4>3.2.3. 增益率 [<sup>目录</sup>](#content)</h4></a>

<p align="center"><img src=./picture/MachineLearning-ZZH-28.jpg width=800 /></p>

<a name="3-gini-index"><h4>3.2.4. 基尼指数 [<sup>目录</sup>](#content)</h4></a>

<p align="center"><img src=./picture/MachineLearning-ZZH-29.jpg width=800 /></p>

<a name="3-pruning"><h3>3.3. 剪枝处理 [<sup>目录</sup>](#content)</h3></a>

<p align="center"><img src=./picture/MachineLearning-ZZH-30.jpg width=800 /></p>

<a name="3-prepruning"><h4>3.3.1. 预剪枝 [<sup>目录</sup>](#content)</h4></a>

<p align="center"><img src=./picture/MachineLearning-ZZH-31.jpg width=800 /></p>

<a name="3-post-pruning"><h4>3.3.2. 后剪枝 [<sup>目录</sup>](#content)</h4></a>

<p align="center"><img src=./picture/MachineLearning-ZZH-32.jpg width=800 /></p>

<p align="center"><img src=./picture/MachineLearning-ZZH-33_1.jpg width=800 /></p>

<a name="3-continuous-and-missing-value"><h3>3.4. 连续与缺失值 [<sup>目录</sup>](#content)</h3></a>

<a name="3-continuous-value"><h4>3.4.1. 连续值处理 [<sup>目录</sup>](#content)</h4></a>

<p align="center"><img src=./picture/MachineLearning-ZZH-33_2.jpg width=800 /></p>

<a name="3-missing-value"><h4>3.4.2. 缺失值处理 [<sup>目录</sup>](#content)</h4></a>

<p align="center"><img src=./picture/MachineLearning-ZZH-34_1.jpg width=800 /></p>

<a name="3-issue1-for-missing-value"><h4>3.4.2.1. 问题(1)划分属性选择 [<sup>目录</sup>](#content)</h4></a>

<p align="center"><img src=./picture/MachineLearning-ZZH-34_2.jpg width=800 /></p>

<p align="center"><img src=./picture/MachineLearning-ZZH-35_1.jpg width=800 /></p>

<a name="3-issue2-for-missing-value"><h4>3.4.2.2. 问题(2)属性值缺失样本的划分 [<sup>目录</sup>](#content)</h4></a>

<p align="center"><img src=./picture/MachineLearning-ZZH-35_2.jpg width=800 /></p>
