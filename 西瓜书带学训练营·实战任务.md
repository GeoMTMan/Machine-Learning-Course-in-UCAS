<a name="content">目录</a>

[西瓜书带学训练营·实战任务](#title)
- [1. “达观杯”文本智能处理挑战赛：sklearn包中logistic算法的使用](#sklearn-lr)
	- [1.1. 任务描述](#sklearn-lr-task-description)
	- [1.2. 编程实现](#sklearn-lr-code)
	- [1.3. sklearn包中logistic算法的使用](#sklearn-lr-usage)
- [2. 鸢尾花数据分类：sklearn包中决策树算法类库的使用](#sklearn-decision-tree)
	- [2.1. DecisionTreeClassifier实例](#sklearn-decision-tree-decisiontreeclassifier-example)
	- [2.2. 可视化决策树](#sklearn-decision-tree-visualize)
- [3. sklearn中随机测试数据：sklearn包中SVM算法库的使用](#sklearn-svm)
	- [3.1. SVM相关知识点回顾](#svm-basic-knowledge)
		- [3.1.1. SVM与SVR](#svm-basic-knowledge-svm-and-svr)
		- [3.1.2. 核函数](#svm-basic-knowledge-svm-kernal)
	- [3.2. sklearn中SVM相关库的简介](#sklearn-svm-introduction)
		- [3.2.1. 分类库与回归库](#sklearn-svm-lib-for-classfication-and-regression)
		- [3.2.2. 高斯核调参](#sklearn-svm-parameters-setting-for-guassian-kernal)
			- [3.2.2.1. 需要调节的参数](#sklearn-svm-parameters-setting-for-guassian-kernal-what-parameters)
			- [3.2.2.2. 调参方法：网格搜索](#sklearn-svm-parameters-setting-for-guassian-kernal-how-to-set-parameters)
	- [3.3. 编程实现](#sklearn-svm-code)


<h1 name="title">西瓜书带学训练营·实战任务</h1>

<a name="sklearn-lr"><h2>1. “达观杯”文本智能处理挑战赛：sklearn包中logistic算法的使用 [<sup>目录</sup>](#content)</h2></a>

<a name="sklearn-lr-task-description"><h3>1.1. 任务描述 [<sup>目录</sup>](#content)</h3></a>

具体任务可以到 ['达观杯'文本智能处理挑战赛官网](http://www.dcjingsai.com/common/cmpt/%E2%80%9C%E8%BE%BE%E8%A7%82%E6%9D%AF%E2%80%9D%E6%96%87%E6%9C%AC%E6%99%BA%E8%83%BD%E5%A4%84%E7%90%86%E6%8C%91%E6%88%98%E8%B5%9B_%E8%B5%9B%E4%BD%93%E4%B8%8E%E6%95%B0%E6%8D%AE.html) 查看

**任务**：建立模型通过长文本数据正文(article)，预测文本对应的类别(class)

数据：

> 数据包含2个csv文件：
> 
> - **train_set.csv**
> 
> 	此数据集用于训练模型，每一行对应一篇文章。文章分别在“字”和“词”的级别上做了脱敏处理。共有四列：
> 
> 	第一列是文章的索引(id)，第二列是文章正文在“字”级别上的表示，即字符相隔正文(article)；第三列是在“词”级别上的表示，即词语相隔正文(word_seg)；第四列是这篇文章的标注(class)。
> 
> 	注：每一个数字对应一个“字”，或“词”，或“标点符号”。“字”的编号与“词”的编号是独立的！
> 
> - **test_set.csv** 
> 
> 	此数据用于测试。数据格式同train_set.csv，但不包含class
> 	
> 	注：test_set与train_test中文章id的编号是独立的。

<a name="sklearn-lr-code"><h3>1.2. 编程实现 [<sup>目录</sup>](#content)</h3></a>

使用**logistic回归**来实现这个多元分类任务

```
import pandas as pd
from sklearn.feature_extract.txt import TfidfVectorizer
from sklearn.decomposition import TruncatedSVD
from sklearn.linear_model import LogisticRegression
from sklearn.model_select import train_test_split
from sklearn.externals import joblib
import time

t_start = time.time()

# ===========================================================
# 1. 载人数据与数据预处理
df_train = pd.read_csv('data/train_set.csv')
df_train = df_train.drop(columns='article',inplace=True)
df_test = pd.read_csv('data/test_set.csv')
df_test = df_test.drop(columns='article',inplace=True)
y_train = (df_train['class'] - 1).values


# ===========================================================
# 2. 特征工程，这里先使用经典的文本特征提取方法TFIDF，提取的TFIDF特征
vectorizer = TfidfVectorizer(ngram_range=(1, 2), min_df=3, max_df=0.9, sublinear_tf=True)
vectorizer.fit(df_train['word_seg'])
X_train = vectorizer.transform(df_train['word_seg'])
X_test = vectorizer.transform(df_test['word_seg'])

# 可以将得到的TFIDF特征保存至本地
'''
data = (X_train,y_train,X_test)
f = open('data/data_tfidf.pkl','wb')
pickle.dump(data,f)
f.close()
'''


# ===========================================================
# 3. 特征降维，将上一步提取的TFIDF特征使用lsa方法进行特征降维
lsa = TruncatedSVD(n_components=200)
X_train = lsa.transform(X_train)
X_test = lsa.transform(X_test)


# ===========================================================
# 4. 训练分类器

# 划分训练集和验证集
X_train,X_vali,y_train,y_vali = train_test_split(X_train,y_train,test_size=0.1,random_state=0)

##  multi_class:分类方式选择参数，有"ovr(默认)"和"multinomial"两个值可选择，在二元逻辑回归中无区别
##  solver:优化算法选择参数，当penalty为"l1"时，参数只能是"liblinear(坐标轴下降法)"；"lbfgs"和"cg"都是关于目标函数的二阶泰勒展开
##  当penalty为"l2"时，参数可以是"lbfgs(拟牛顿法)","newton_cg(牛顿法变种)","seg(minibactch随机平均梯度下降)"
lr = LogisticRegression(multi_class="ovr",penalty="l2",solver="lbfgs")
lr.fit(X_train,y_train)

# 模型的保存与持久化
joblib.dump(lr,"logistic_lr.model")
joblib.load("logistic_lr.model") #加载模型,会保存该model文件


# ===========================================================
# 5. 在验证集上评估模型
pre_vali = lr.predict(X_vali)
pre_score = f1_score(y_true=y_vali,y_pred=pre_vali,average='macro')
print("验证集分数：{}".format(score_vali))


# ===========================================================
# 6. 对测试集进行预测
y_test = lr.predict(X_test) + 1
```

<a name="sklearn-lr-usage"><h3>1.3. sklearn包中logistic算法的使用 [<sup>目录</sup>](#content)</h3></a>

<p align="center"><img src=./picture/InAction-MachineLearning-XGS-sklearn-lr-1.png width=800 /></p>

<p align="center"><img src=./picture/InAction-MachineLearning-XGS-sklearn-lr-2.png width=800 /></p>

<p align="center"><img src=./picture/InAction-MachineLearning-XGS-sklearn-lr-3.png width=800 /></p>

<p align="center"><img src=./picture/InAction-MachineLearning-XGS-sklearn-lr-4.png width=800 /></p>

<p align="center"><img src=./picture/InAction-MachineLearning-XGS-sklearn-lr-5.png width=800 /></p>

<p align="center"><img src=./picture/InAction-MachineLearning-XGS-sklearn-lr-6.png width=800 /></p>

<p align="center"><img src=./picture/InAction-MachineLearning-XGS-sklearn-lr-7.png width=800 /></p>

<a name="sklearn-decision-tree"><h2>2. 鸢尾花数据分类：sklearn包中决策树算法类库的使用 [<sup>目录</sup>](#content)</h2></a>

<a name="sklearn-decision-tree-decisiontreeclassifier-example"><h3>2.1. DecisionTreeClassifier实例 [<sup>目录</sup>](#content)</h3></a>

```
from itertools import product

import numpy as np
import matplotlib.pyplot as plt

from sklearn import datasets
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2
from sklearn.decomposition import PCA


# 使用自带的iris数据
iris = datasets.load_iris()
X = iris.data[:, np.arange(0,4)]
y = iris.target

# 划分训练集与测试集
x_train, x_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=14)
print("训练数据集样本总数:%d;测试数据集样本总数:%d" %(x_train.shape[0],x_test.shape[0]))

# 对数据集进行标准化
ss = MinMaxScaler()
x_train = ss.fit_transform(x_train,y_train)
x_test = ss.transform(x_test)

# 特征选择：从已有的特征属性中选择出影响目标最大的特征属性
# 常用方法：
#	离散属性：F统计量、卡方系数、互信息mutual_info_classif
#	连续属性：皮尔逊相关系数、F统计量、互信息mutual_info_classif}
# 这里使用离散属性的卡方系数，实现函数为SelectKBest，用SelectKBest方法从四个原始特征属性中选择出最能影响目标的3个特征属性
ch2 = SelectKBest(chi2,k=3) # k默认为10，指定后会返回想要的特征个数
ch2.fit(x_train,y_train)
x_train = ch2.transform(x_train)
x_test = ch2.transform(x_test)

# 特征降维，这里使用PCA方法
pca = PCA(n_components=2)   # 构建一个PCA对象，设置最终维度为2维。这里为了后边画图方便，将数据维度设置为 2，一般用默认不设置就可以
x_train = pca.fit_transform(x_train) # 训练与转换，也可以拆分成两步
x_test = pca.transform(x_test)

# 训练模型
#	criterion：指定特征选择标准，可以使用"gini"或者"entropy"，前者代表基尼系数，后者代表信息增益
#	max_depth：限制树的最大深度4
clf = DecisionTreeClassifier(criterion="entropy",max_depth=4)
clf.fit(X, y) # 拟合模型


# 画图
x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
# 生成网格采样点
xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),
                     np.arange(y_min, y_max, 0.1))

Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)

plt.contourf(xx, yy, Z, alpha=0.4)
plt.scatter(X[:, 0], X[:, 1], c=y, alpha=0.8)
plt.show()
```

<p align="center"><img src=./picture/InAction-MachineLearning-XGS-sklearn-decision-tree-1.png width=800 /></p>

<a name="sklearn-decision-tree-visualize"><h3>2.2. 可视化决策树 [<sup>目录</sup>](#content)</h3></a>

scikit-learn中决策树的可视化一般需要安装graphviz，主要包括graphviz的安装和python的graphviz插件的安装

> - 1) 安装graphviz。下载地址在：`http://www.graphviz.org`/。如果你是linux，可以用apt-get或者yum的方法安装。如果是windows，就在官网下载msi文件安装。无论是linux还是windows，装完后都要设置环境变量，将graphviz的bin目录加到PATH，比如我是windows，将`C:/Program Files (x86)/Graphviz2.38/bin/`加入了`PATH`
> 
> - 2) 安装python插件graphviz： `pip install graphviz`
> 
> 
> - 3) 安装python插件pydotplus: `pip install pydotplus`

这样环境就搭好了，有时候python会很笨，仍然找不到graphviz，这时，可以在代码里面加入这一行：

```
os.environ["PATH"] += os.pathsep + 'C:/Program Files (x86)/Graphviz2.38/bin/'
```

可视化决策树的代码：

```
from IPython.display import Image  
from sklearn import tree
import pydotplus 
dot_data = tree.export_graphviz(clf, out_file=None, 
                         feature_names=iris.feature_names,  
                         class_names=iris.target_names,  
                         filled=True, rounded=True,  
                         special_characters=True)  
graph = pydotplus.graph_from_dot_data(dot_data)  
Image(graph.create_png())
```

<p align="center"><img src=./picture/InAction-MachineLearning-XGS-sklearn-decision-tree-2.png width=800 /></p>

<a name="sklearn-svm"><h2>3. sklearn中随机测试数据：sklearn包中SVM算法库的使用 [<sup>目录</sup>](#content)</h2></a>

<a name="svm-basic-knowledge"><h3>3.1. SVM相关知识点回顾 [<sup>目录</sup>](#content)</h3></a>

<a name="svm-basic-knowledge-svm-and-svr"><h4>3.1.1. SVM与SVR [<sup>目录</sup>](#content)</h4></a>

- **SVM分类算法**

	其原始形式是：
	
	<p align="center"><img src=./picture/InAction-MachineLearning-XGS-sklearn-SVM-1.png width=400 /></p>
	
	其中m为样本个数，我们的样本为(x<sub>1</sub>,y<sub>1</sub>),(x<sub>2</sub>,y<sub>2</sub>),...,(x<sub>m</sub>,y<sub>m</sub>)。w,b是我们的分离超平面的w∙ϕ(x<sub>i</sub>)+b=0系数, ξ<sub>i</sub>为第i个样本的松弛系数， C为惩罚系数。ϕ(x<sub>i</sub>)为低维到高维的映射函数
	
	通过拉格朗日函数以及对偶化后的形式为：
	
	<p align="center"><img src=./picture/InAction-MachineLearning-XGS-sklearn-SVM-2.png width=400 /></p>

- **SVR回归算法**

	<p align="center"><img src=./picture/InAction-MachineLearning-XGS-sklearn-SVM-3.png width=400 /></p>
	
	其中m为样本个数，我们的样本为(x<sub>1</sub>,y<sub>1</sub>),(x<sub>2</sub>,y<sub>2</sub>),...,(x<sub>m</sub>,y<sub>m</sub>)。w,b是我们的回归超平面的w∙x<sub>i</sub>+b=0系数, ξ<sup>∨</sup><sub>i</sub>，ξ<sup>∧</sup><sub>i</sub>为第i个样本的松弛系数， C为惩罚系数，ϵ为损失边界，到超平面距离小于ϵ的训练集的点没有损失。ϕ(x<sub>i</sub>)为低维到高维的映射函数。
	
	<p align="center"><img src=./picture/InAction-MachineLearning-XGS-sklearn-SVM-4.png width=600 /></p>


<a name="svm-basic-knowledge-svm-kernal"><h4>3.1.2. 核函数 [<sup>目录</sup>](#content)</h4></a>

在scikit-learn中，内置的核函数一共有4种：

- **线性核函数**（Linear Kernel）表达式为：K(x,z)=x∙z，就是普通的内积

- **多项式核函数**（Polynomial Kernel）是线性不可分SVM常用的核函数之一，表达式为：K(x,z)=（γx∙z+r)<sup>d</sup> ，其中，γ,r,d都需要自己调参定义

- **高斯核函数**（Gaussian Kernel），在SVM中也称为径向基核函数（Radial Basis Function,RBF），它是 libsvm 默认的核函数，当然也是 scikit-learn 默认的核函数。表达式为：K(x,z)=exp(−γ||x−z||<sup>2</sup>)， 其中，γ大于0，需要自己调参定义

- **Sigmoid核函数**（Sigmoid Kernel）也是线性不可分SVM常用的核函数之一，表达式为：K(x,z)=tanh（γx∙z+r)， 其中，γ，r都需要自己调参定义

一般情况下，对非线性数据使用默认的高斯核函数会有比较好的效果，如果你不是SVM调参高手的话，建议使用高斯核来做数据分析。　

<a name="sklearn-svm-introduction"><h3>3.2. sklearn中SVM相关库的简介 [<sup>目录</sup>](#content)</h3></a>

scikit-learn SVM算法库封装了libsvm 和 liblinear 的实现，仅仅重写了算法了接口部分

<a name="sklearn-svm-lib-for-classfication-and-regression"><h4>3.2.1. 分类库与回归库 [<sup>目录</sup>](#content)</h4></a>

- **分类算法库**

	包括SVC， NuSVC，和LinearSVC 3个类
	
	对于SVC， NuSVC，和LinearSVC 3个分类的类，SVC和 NuSVC差不多，区别仅仅在于对损失的度量方式不同，而LinearSVC从名字就可以看出，他是线性分类，也就是不支持各种低维到高维的核函数，仅仅支持线性核函数，对线性不可分的数据不能使用

- **回归算法库**

	包括SVR， NuSVR，和LinearSVR 3个类
	
	同样的，对于SVR， NuSVR，和LinearSVR 3个回归的类， SVR和NuSVR差不多，区别也仅仅在于对损失的度量方式不同。LinearSVR是线性回归，只能使用线性核函数

<a name="sklearn-svm-parameters-setting-for-guassian-kernal"><h4>3.2.2. 高斯核调参 [<sup>目录</sup>](#content)</h4></a>

<a name="sklearn-svm-parameters-setting-for-guassian-kernal-what-parameters"><h5>3.2.2.1. 需要调节的参数 [<sup>目录</sup>](#content)</h5></a>

- **SVM分类模型**

	如果是SVM分类模型，这两个超参数分别是**惩罚系数C**和**RBF核函数的系数γ**
	
	**惩罚系数C**
	
	> 它在优化函数里主要是平衡支持向量的复杂度和误分类率这两者之间的关系，可以理解为正则化系数
	> 
	> - 当C比较大时，我们的损失函数也会越大，这意味着我们不愿意放弃比较远的离群点。这样我们会有更加多的支持向量，也就是说支持向量和超平面的模型也会变得越复杂，也容易过拟合
	> 
	> - 当C比较小时，意味我们不想理那些离群点，会选择较少的样本来做支持向量，最终的支持向量和超平面的模型也会简单
	> 
	> scikit-learn中默认值是1

	C越大，泛化能力越差，易出现过拟合现象；C越小，泛化能力越好，易出现过欠拟合现象

	**BF核函数的参数γ**

	> RBF 核函数K(x,z)=exp(−γ||x−z||<sup>2</sup>) γ>0
	> 
	> γ主要定义了单个样本对整个分类超平面的影响
	> 
	> - 当γ比较小时，单个样本对整个分类超平面的影响比较小，不容易被选择为支持向量
	> 
	> - 当γ比较大时，单个样本对整个分类超平面的影响比较大，更容易被选择为支持向量，或者说整个模型的支持向量也会多
	> 
	> scikit-learn中默认值是 `1/样本特征数`

	γ越大，训练集拟合越好，泛化能力越差，易出现过拟合现象

	如果把惩罚系数C和RBF核函数的系数γ一起看，当C比较大， γ比较大时，我们会有更多的支持向量，我们的模型会比较复杂，容易过拟合一些。如果C比较小 ， γ比较小时，模型会变得简单，支持向量的个数会少

- **SVM回归模型**

	SVM回归模型的RBF核比分类模型要复杂一点，因为此时我们除了惩罚系数C和RBF核函数的系数γ之外，还多了一个**损失距离度量ϵ**

	> 对于损失距离度量ϵ，它决定了样本点到超平面的距离损失
	> 
	> - 当 ϵ 比较大时，损失较小，更多的点在损失距离范围之内，而没有损失,模型较简单
	> 
	> - 当 ϵ 比较小时，损失函数会较大，模型也会变得复杂
	> 
	> scikit-learn中默认值是0.1

	如果把惩罚系数C，RBF核函数的系数γ和损失距离度量ϵ一起看，当C比较大， γ比较大，ϵ比较小时，我们会有更多的支持向量，我们的模型会比较复杂，容易过拟合一些。如果C比较小 ， γ比较小，ϵ比较大时，模型会变得简单，支持向量的个数会少

<a name="sklearn-svm-parameters-setting-for-guassian-kernal-how-to-set-parameters"><h5>3.2.2.2. 调参方法：网格搜索 [<sup>目录</sup>](#content)</h5></a>

对于SVM的RBF核，我们主要的调参方法都是交叉验证。具体在scikit-learn中，主要是使用网格搜索，即GridSearchCV类

```
from sklearn.model_selection import GridSearchCV
grid = GridSearchCV(SVC(), param_grid={"C":[0.1, 1, 10], "gamma": [1, 0.1, 0.01]}, cv=4)
grid.fit(X, y)
```

将GridSearchCV类用于SVM RBF调参时要注意的参数有：

> 1) **estimator**：即我们的模型，此处我们就是带高斯核的SVC或者SVR
> 
> 2) **param_grid**：即我们要调参的参数列表。 比如我们用SVC分类模型的话，那么param_grid可以定义为{"C":[0.1, 1, 10], "gamma": [0.1, 0.2, 0.3]}，这样我们就会有9种超参数的组合来进行网格搜索，选择一个拟合分数最好的超平面系数
> 
> 3) **cv**：S折交叉验证的折数，即将训练集分成多少份来进行交叉验证。默认是3。如果样本较多的话，可以适度增大cv的值

<a name="sklearn-svm-code"><h3>3.3. 编程实现 [<sup>目录</sup>](#content)</h3></a>

1. 生成测试数据

	```
	from sklearn.datasets import make_circles
	from sklearn.preprocessing import StandardScaler
	
	# 生成一些随机数据用于后续分类
	X, y = make_circles(noise=0.2, factor=0.5, random_state=1) # 生成时加入了一些噪声
	X = StandardScaler().fit_transform(X) # 把数据归一化
	
	```
	
	生成的随机数据可视化结果如下：
	
	<p align="center"><img src=./picture/InAction-MachineLearning-XGS-sklearn-SVM-5.png width=600 /></p>

2. 调参

	接着采用网格搜索的策略进行RBF核函数参数搜索
	
	```
	from sklearn.model_selection import GridSearchCV
	grid = GridSearchCV(SVC(), param_grid={"C":[0.1, 1, 10], "gamma": [1, 0.1, 0.01]}, cv=4) # 总共有9种参数组合的搜索空间
	grid.fit(X, y)
	print("The best parameters are %s with a score of %0.2f"
	      % (grid.best_params_, grid.best_score_))
	
	输出为：
	The best parameters are {'C': 10, 'gamma': 0.1} with a score of 0.91
	```

	可以对9种参数组合训练的结果进行可视化，观察分类的效果：

```
x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max,0.02),
                     np.arange(y_min, y_max, 0.02))

for i, C in enumerate((0.1, 1, 10)):
    for j, gamma in enumerate((1, 0.1, 0.01)):
        plt.subplot()       
        clf = SVC(C=C, gamma=gamma)
        clf.fit(X,y)
        Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])

        # Put the result into a color plot
        Z = Z.reshape(xx.shape)
        plt.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.8)

        # Plot also the training points
        plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm)

        plt.xlim(xx.min(), xx.max())
        plt.ylim(yy.min(), yy.max())
        plt.xticks(())
        plt.yticks(())
        plt.xlabel(" gamma=" + str(gamma) + " C=" + str(C))
        plt.show()
```

| `C \ gamma` | 1 | 0.1 | 0.001 |
|:---:|:---:|:---:|:---:|
| 0.1 | <img src=./picture/InAction-MachineLearning-XGS-sklearn-SVM-6.png width=300 />| <img src=./picture/InAction-MachineLearning-XGS-sklearn-SVM-7.png width=300 /> | <img src=./picture/InAction-MachineLearning-XGS-sklearn-SVM-8.png width=300 /> |
| 1 | <img src=./picture/InAction-MachineLearning-XGS-sklearn-SVM-9.png width=300 /> | <img src=./picture/InAction-MachineLearning-XGS-sklearn-SVM-10.png width=300 /> | <img src=./picture/InAction-MachineLearning-XGS-sklearn-SVM-11.png width=300 /> |
| 10 | <img src=./picture/InAction-MachineLearning-XGS-sklearn-SVM-12.png width=300 /> | <img src=./picture/InAction-MachineLearning-XGS-sklearn-SVM-13.png width=300 /> | <img src=./picture/InAction-MachineLearning-XGS-sklearn-SVM-14.png width=300 /> |


---

参考资料：

(1) [【GitHub】MLjian/TextClassificationImplement](https://github.com/MLjian/TextClassificationImplement)

(2) [刘建平Pinard《scikit-learn 逻辑回归类库使用小结》](https://www.cnblogs.com/pinard/p/6035872.html)

(3) [机器学习sklearn19.0——Logistic回归算法](https://blog.csdn.net/loveliuzz/article/details/78708359)

(4) [刘建平Pinard《scikit-learn决策树算法类库使用小结》](https://www.cnblogs.com/pinard/p/6056319.html)

(6) [刘建平Pinard《scikit-learn 支持向量机算法库使用小结》](https://www.cnblogs.com/pinard/p/6117515.html)

(7) [刘建平Pinard《支持向量机高斯核调参小结》](https://www.cnblogs.com/pinard/p/6126077.html)

(8) [loveliuzz《机器学习sklearn19.0——SVM算法》](https://blog.csdn.net/loveliuzz/article/details/78768063)

