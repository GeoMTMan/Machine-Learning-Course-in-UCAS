<a name="content">目录</a>

[西瓜书带学训练营·实战任务](#title)
- [1. “达观杯”文本智能处理挑战赛：sklearn包中logistic算法的使用](#sklearn-lr)
	- [1.1. 任务描述](#sklearn-lr-task-description)
	- [1.2. 编程实现](#sklearn-lr-code)
	- [1.3. sklearn包中logistic算法的使用](#sklearn-lr-usage)
- [2. 鸢尾花数据分类：sklearn包中决策树算法类库的使用](#sklearn-decision-tree)
	- [2.1. DecisionTreeClassifier实例](#sklearn-decision-tree-decisiontreeclassifier-example)
	- [2.2. 可视化决策树](#sklearn-decision-tree-visualize)
- [3. sklearn中随机测试数据：sklearn包中SVM算法库的使用](#sklearn-svm)
	- [3.1. SVM相关知识点回顾](#svm-basic-knowledge)
		- [3.1.1. SVM与SVR](#svm-basic-knowledge-svm-and-svr)
		- [3.1.2. 核函数](#svm-basic-knowledge-svm-kernal)
	- [3.2. sklearn中SVM相关库的简介](#sklearn-svm-introduction)
		- [3.2.1. 分类库与回归库](#sklearn-svm-lib-for-classfication-and-regression)
		- [3.2.2. 高斯核调参](#sklearn-svm-parameters-setting-for-guassian-kernal)
			- [3.2.2.1. 需要调节的参数](#sklearn-svm-parameters-setting-for-guassian-kernal-what-parameters)
			- [3.2.2.2. 调参方法：网格搜索](#sklearn-svm-parameters-setting-for-guassian-kernal-how-to-set-parameters)
	- [3.3. 编程实现](#sklearn-svm-code)
- [4.sklearn包中朴素贝叶斯库的使用](#sklearn-naive-bayes)
	- [4.1. 朴素贝叶斯相关知识点回顾](#naive-bayes-basic-knowledge)
		- [4.1.1. 什么是朴素贝叶斯分类器](#what-is-naive-bayes)
		- [4.1.2. 朴素贝叶斯推断](#naive-bayes-inference)
		- [4.1.3. 朴素贝叶斯学习](#naive-bayes-learning)
	- [4.2. sklearn中朴素贝叶斯类库的简介](#sklearn-naive-bayes-introduction)
		- [4.2.1. GaussianNB类](#gaussian-nb)
		- [4.2.2. MultinomialNB类](#multinomial-nb)
		- [4.2.3. BernoulliNB类](#bernoulli-nb)

<h1 name="title">西瓜书带学训练营·实战任务</h1>

<a name="sklearn-lr"><h2>1. “达观杯”文本智能处理挑战赛：sklearn包中logistic算法的使用 [<sup>目录</sup>](#content)</h2></a>

<a name="sklearn-lr-task-description"><h3>1.1. 任务描述 [<sup>目录</sup>](#content)</h3></a>

具体任务可以到 ['达观杯'文本智能处理挑战赛官网](http://www.dcjingsai.com/common/cmpt/%E2%80%9C%E8%BE%BE%E8%A7%82%E6%9D%AF%E2%80%9D%E6%96%87%E6%9C%AC%E6%99%BA%E8%83%BD%E5%A4%84%E7%90%86%E6%8C%91%E6%88%98%E8%B5%9B_%E8%B5%9B%E4%BD%93%E4%B8%8E%E6%95%B0%E6%8D%AE.html) 查看

**任务**：建立模型通过长文本数据正文(article)，预测文本对应的类别(class)

数据：

> 数据包含2个csv文件：
> 
> - **train_set.csv**
> 
> 	此数据集用于训练模型，每一行对应一篇文章。文章分别在“字”和“词”的级别上做了脱敏处理。共有四列：
> 
> 	第一列是文章的索引(id)，第二列是文章正文在“字”级别上的表示，即字符相隔正文(article)；第三列是在“词”级别上的表示，即词语相隔正文(word_seg)；第四列是这篇文章的标注(class)。
> 
> 	注：每一个数字对应一个“字”，或“词”，或“标点符号”。“字”的编号与“词”的编号是独立的！
> 
> - **test_set.csv** 
> 
> 	此数据用于测试。数据格式同train_set.csv，但不包含class
> 	
> 	注：test_set与train_test中文章id的编号是独立的。

<a name="sklearn-lr-code"><h3>1.2. 编程实现 [<sup>目录</sup>](#content)</h3></a>

使用**logistic回归**来实现这个多元分类任务

```
import pandas as pd
from sklearn.feature_extract.txt import TfidfVectorizer
from sklearn.decomposition import TruncatedSVD
from sklearn.linear_model import LogisticRegression
from sklearn.model_select import train_test_split
from sklearn.externals import joblib
import time

t_start = time.time()

# ===========================================================
# 1. 载人数据与数据预处理
df_train = pd.read_csv('data/train_set.csv')
df_train = df_train.drop(columns='article',inplace=True)
df_test = pd.read_csv('data/test_set.csv')
df_test = df_test.drop(columns='article',inplace=True)
y_train = (df_train['class'] - 1).values


# ===========================================================
# 2. 特征工程，这里先使用经典的文本特征提取方法TFIDF，提取的TFIDF特征
vectorizer = TfidfVectorizer(ngram_range=(1, 2), min_df=3, max_df=0.9, sublinear_tf=True)
vectorizer.fit(df_train['word_seg'])
X_train = vectorizer.transform(df_train['word_seg'])
X_test = vectorizer.transform(df_test['word_seg'])

# 可以将得到的TFIDF特征保存至本地
'''
data = (X_train,y_train,X_test)
f = open('data/data_tfidf.pkl','wb')
pickle.dump(data,f)
f.close()
'''


# ===========================================================
# 3. 特征降维，将上一步提取的TFIDF特征使用lsa方法进行特征降维
lsa = TruncatedSVD(n_components=200)
X_train = lsa.transform(X_train)
X_test = lsa.transform(X_test)


# ===========================================================
# 4. 训练分类器

# 划分训练集和验证集
X_train,X_vali,y_train,y_vali = train_test_split(X_train,y_train,test_size=0.1,random_state=0)

##  multi_class:分类方式选择参数，有"ovr(默认)"和"multinomial"两个值可选择，在二元逻辑回归中无区别
##  solver:优化算法选择参数，当penalty为"l1"时，参数只能是"liblinear(坐标轴下降法)"；"lbfgs"和"cg"都是关于目标函数的二阶泰勒展开
##  当penalty为"l2"时，参数可以是"lbfgs(拟牛顿法)","newton_cg(牛顿法变种)","seg(minibactch随机平均梯度下降)"
lr = LogisticRegression(multi_class="ovr",penalty="l2",solver="lbfgs")
lr.fit(X_train,y_train)

# 模型的保存与持久化
joblib.dump(lr,"logistic_lr.model")
joblib.load("logistic_lr.model") #加载模型,会保存该model文件


# ===========================================================
# 5. 在验证集上评估模型
pre_vali = lr.predict(X_vali)
pre_score = f1_score(y_true=y_vali,y_pred=pre_vali,average='macro')
print("验证集分数：{}".format(score_vali))


# ===========================================================
# 6. 对测试集进行预测
y_test = lr.predict(X_test) + 1
```

<a name="sklearn-lr-usage"><h3>1.3. sklearn包中logistic算法的使用 [<sup>目录</sup>](#content)</h3></a>

<p align="center"><img src=./picture/InAction-MachineLearning-XGS-sklearn-lr-1.png width=800 /></p>

<p align="center"><img src=./picture/InAction-MachineLearning-XGS-sklearn-lr-2.png width=800 /></p>

<p align="center"><img src=./picture/InAction-MachineLearning-XGS-sklearn-lr-3.png width=800 /></p>

<p align="center"><img src=./picture/InAction-MachineLearning-XGS-sklearn-lr-4.png width=800 /></p>

<p align="center"><img src=./picture/InAction-MachineLearning-XGS-sklearn-lr-5.png width=800 /></p>

<p align="center"><img src=./picture/InAction-MachineLearning-XGS-sklearn-lr-6.png width=800 /></p>

<p align="center"><img src=./picture/InAction-MachineLearning-XGS-sklearn-lr-7.png width=800 /></p>

<a name="sklearn-decision-tree"><h2>2. 鸢尾花数据分类：sklearn包中决策树算法类库的使用 [<sup>目录</sup>](#content)</h2></a>

<a name="sklearn-decision-tree-decisiontreeclassifier-example"><h3>2.1. DecisionTreeClassifier实例 [<sup>目录</sup>](#content)</h3></a>

```
from itertools import product

import numpy as np
import matplotlib.pyplot as plt

from sklearn import datasets
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2
from sklearn.decomposition import PCA


# 使用自带的iris数据
iris = datasets.load_iris()
X = iris.data[:, np.arange(0,4)]
y = iris.target

# 划分训练集与测试集
x_train, x_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=14)
print("训练数据集样本总数:%d;测试数据集样本总数:%d" %(x_train.shape[0],x_test.shape[0]))

# 对数据集进行标准化
ss = MinMaxScaler()
x_train = ss.fit_transform(x_train,y_train)
x_test = ss.transform(x_test)

# 特征选择：从已有的特征属性中选择出影响目标最大的特征属性
# 常用方法：
#	离散属性：F统计量、卡方系数、互信息mutual_info_classif
#	连续属性：皮尔逊相关系数、F统计量、互信息mutual_info_classif}
# 这里使用离散属性的卡方系数，实现函数为SelectKBest，用SelectKBest方法从四个原始特征属性中选择出最能影响目标的3个特征属性
ch2 = SelectKBest(chi2,k=3) # k默认为10，指定后会返回想要的特征个数
ch2.fit(x_train,y_train)
x_train = ch2.transform(x_train)
x_test = ch2.transform(x_test)

# 特征降维，这里使用PCA方法
pca = PCA(n_components=2)   # 构建一个PCA对象，设置最终维度为2维。这里为了后边画图方便，将数据维度设置为 2，一般用默认不设置就可以
x_train = pca.fit_transform(x_train) # 训练与转换，也可以拆分成两步
x_test = pca.transform(x_test)

# 训练模型
#	criterion：指定特征选择标准，可以使用"gini"或者"entropy"，前者代表基尼系数，后者代表信息增益
#	max_depth：限制树的最大深度4
clf = DecisionTreeClassifier(criterion="entropy",max_depth=4)
clf.fit(X, y) # 拟合模型


# 画图
x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
# 生成网格采样点
xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),
                     np.arange(y_min, y_max, 0.1))

Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)

plt.contourf(xx, yy, Z, alpha=0.4)
plt.scatter(X[:, 0], X[:, 1], c=y, alpha=0.8)
plt.show()
```

<p align="center"><img src=./picture/InAction-MachineLearning-XGS-sklearn-decision-tree-1.png width=800 /></p>

<a name="sklearn-decision-tree-visualize"><h3>2.2. 可视化决策树 [<sup>目录</sup>](#content)</h3></a>

scikit-learn中决策树的可视化一般需要安装graphviz，主要包括graphviz的安装和python的graphviz插件的安装

> - 1) 安装graphviz。下载地址在：`http://www.graphviz.org`/。如果你是linux，可以用apt-get或者yum的方法安装。如果是windows，就在官网下载msi文件安装。无论是linux还是windows，装完后都要设置环境变量，将graphviz的bin目录加到PATH，比如我是windows，将`C:/Program Files (x86)/Graphviz2.38/bin/`加入了`PATH`
> 
> - 2) 安装python插件graphviz： `pip install graphviz`
> 
> 
> - 3) 安装python插件pydotplus: `pip install pydotplus`

这样环境就搭好了，有时候python会很笨，仍然找不到graphviz，这时，可以在代码里面加入这一行：

```
os.environ["PATH"] += os.pathsep + 'C:/Program Files (x86)/Graphviz2.38/bin/'
```

可视化决策树的代码：

```
from IPython.display import Image  
from sklearn import tree
import pydotplus 
dot_data = tree.export_graphviz(clf, out_file=None, 
                         feature_names=iris.feature_names,  
                         class_names=iris.target_names,  
                         filled=True, rounded=True,  
                         special_characters=True)  
graph = pydotplus.graph_from_dot_data(dot_data)  
Image(graph.create_png())
```

<p align="center"><img src=./picture/InAction-MachineLearning-XGS-sklearn-decision-tree-2.png width=800 /></p>

<a name="sklearn-svm"><h2>3. sklearn中随机测试数据：sklearn包中SVM算法库的使用 [<sup>目录</sup>](#content)</h2></a>

<a name="svm-basic-knowledge"><h3>3.1. SVM相关知识点回顾 [<sup>目录</sup>](#content)</h3></a>

<a name="svm-basic-knowledge-svm-and-svr"><h4>3.1.1. SVM与SVR [<sup>目录</sup>](#content)</h4></a>

- **SVM分类算法**

	其原始形式是：
	
	<p align="center"><img src=./picture/InAction-MachineLearning-XGS-sklearn-SVM-1.png width=400 /></p>
	
	其中m为样本个数，我们的样本为(x<sub>1</sub>,y<sub>1</sub>),(x<sub>2</sub>,y<sub>2</sub>),...,(x<sub>m</sub>,y<sub>m</sub>)。w,b是我们的分离超平面的w∙ϕ(x<sub>i</sub>)+b=0系数, ξ<sub>i</sub>为第i个样本的松弛系数， C为惩罚系数。ϕ(x<sub>i</sub>)为低维到高维的映射函数
	
	通过拉格朗日函数以及对偶化后的形式为：
	
	<p align="center"><img src=./picture/InAction-MachineLearning-XGS-sklearn-SVM-2.png width=400 /></p>

- **SVR回归算法**

	<p align="center"><img src=./picture/InAction-MachineLearning-XGS-sklearn-SVM-3.png width=400 /></p>
	
	其中m为样本个数，我们的样本为(x<sub>1</sub>,y<sub>1</sub>),(x<sub>2</sub>,y<sub>2</sub>),...,(x<sub>m</sub>,y<sub>m</sub>)。w,b是我们的回归超平面的w∙x<sub>i</sub>+b=0系数, ξ<sup>∨</sup><sub>i</sub>，ξ<sup>∧</sup><sub>i</sub>为第i个样本的松弛系数， C为惩罚系数，ϵ为损失边界，到超平面距离小于ϵ的训练集的点没有损失。ϕ(x<sub>i</sub>)为低维到高维的映射函数。
	
	<p align="center"><img src=./picture/InAction-MachineLearning-XGS-sklearn-SVM-4.png width=600 /></p>


<a name="svm-basic-knowledge-svm-kernal"><h4>3.1.2. 核函数 [<sup>目录</sup>](#content)</h4></a>

在scikit-learn中，内置的核函数一共有4种：

- **线性核函数**（Linear Kernel）表达式为：K(x,z)=x∙z，就是普通的内积

- **多项式核函数**（Polynomial Kernel）是线性不可分SVM常用的核函数之一，表达式为：K(x,z)=（γx∙z+r)<sup>d</sup> ，其中，γ,r,d都需要自己调参定义

- **高斯核函数**（Gaussian Kernel），在SVM中也称为径向基核函数（Radial Basis Function,RBF），它是 libsvm 默认的核函数，当然也是 scikit-learn 默认的核函数。表达式为：K(x,z)=exp(−γ||x−z||<sup>2</sup>)， 其中，γ大于0，需要自己调参定义

- **Sigmoid核函数**（Sigmoid Kernel）也是线性不可分SVM常用的核函数之一，表达式为：K(x,z)=tanh（γx∙z+r)， 其中，γ，r都需要自己调参定义

一般情况下，对非线性数据使用默认的高斯核函数会有比较好的效果，如果你不是SVM调参高手的话，建议使用高斯核来做数据分析。　

<a name="sklearn-svm-introduction"><h3>3.2. sklearn中SVM相关库的简介 [<sup>目录</sup>](#content)</h3></a>

scikit-learn SVM算法库封装了libsvm 和 liblinear 的实现，仅仅重写了算法了接口部分

<a name="sklearn-svm-lib-for-classfication-and-regression"><h4>3.2.1. 分类库与回归库 [<sup>目录</sup>](#content)</h4></a>

- **分类算法库**

	包括SVC， NuSVC，和LinearSVC 3个类
	
	对于SVC， NuSVC，和LinearSVC 3个分类的类，SVC和 NuSVC差不多，区别仅仅在于对损失的度量方式不同，而LinearSVC从名字就可以看出，他是线性分类，也就是不支持各种低维到高维的核函数，仅仅支持线性核函数，对线性不可分的数据不能使用

- **回归算法库**

	包括SVR， NuSVR，和LinearSVR 3个类
	
	同样的，对于SVR， NuSVR，和LinearSVR 3个回归的类， SVR和NuSVR差不多，区别也仅仅在于对损失的度量方式不同。LinearSVR是线性回归，只能使用线性核函数

<a name="sklearn-svm-parameters-setting-for-guassian-kernal"><h4>3.2.2. 高斯核调参 [<sup>目录</sup>](#content)</h4></a>

<a name="sklearn-svm-parameters-setting-for-guassian-kernal-what-parameters"><h5>3.2.2.1. 需要调节的参数 [<sup>目录</sup>](#content)</h5></a>

- **SVM分类模型**

	如果是SVM分类模型，这两个超参数分别是**惩罚系数C**和**RBF核函数的系数γ**
	
	**惩罚系数C**
	
	> 它在优化函数里主要是平衡支持向量的复杂度和误分类率这两者之间的关系，可以理解为正则化系数
	> 
	> - 当C比较大时，我们的损失函数也会越大，这意味着我们不愿意放弃比较远的离群点。这样我们会有更加多的支持向量，也就是说支持向量和超平面的模型也会变得越复杂，也容易过拟合
	> 
	> - 当C比较小时，意味我们不想理那些离群点，会选择较少的样本来做支持向量，最终的支持向量和超平面的模型也会简单
	> 
	> scikit-learn中默认值是1

	C越大，泛化能力越差，易出现过拟合现象；C越小，泛化能力越好，易出现过欠拟合现象

	**BF核函数的参数γ**

	> RBF 核函数K(x,z)=exp(−γ||x−z||<sup>2</sup>) γ>0
	> 
	> γ主要定义了单个样本对整个分类超平面的影响
	> 
	> - 当γ比较小时，单个样本对整个分类超平面的影响比较小，不容易被选择为支持向量
	> 
	> - 当γ比较大时，单个样本对整个分类超平面的影响比较大，更容易被选择为支持向量，或者说整个模型的支持向量也会多
	> 
	> scikit-learn中默认值是 `1/样本特征数`

	γ越大，训练集拟合越好，泛化能力越差，易出现过拟合现象

	如果把惩罚系数C和RBF核函数的系数γ一起看，当C比较大， γ比较大时，我们会有更多的支持向量，我们的模型会比较复杂，容易过拟合一些。如果C比较小 ， γ比较小时，模型会变得简单，支持向量的个数会少

- **SVM回归模型**

	SVM回归模型的RBF核比分类模型要复杂一点，因为此时我们除了惩罚系数C和RBF核函数的系数γ之外，还多了一个**损失距离度量ϵ**

	> 对于损失距离度量ϵ，它决定了样本点到超平面的距离损失
	> 
	> - 当 ϵ 比较大时，损失较小，更多的点在损失距离范围之内，而没有损失,模型较简单
	> 
	> - 当 ϵ 比较小时，损失函数会较大，模型也会变得复杂
	> 
	> scikit-learn中默认值是0.1

	如果把惩罚系数C，RBF核函数的系数γ和损失距离度量ϵ一起看，当C比较大， γ比较大，ϵ比较小时，我们会有更多的支持向量，我们的模型会比较复杂，容易过拟合一些。如果C比较小 ， γ比较小，ϵ比较大时，模型会变得简单，支持向量的个数会少

<a name="sklearn-svm-parameters-setting-for-guassian-kernal-how-to-set-parameters"><h5>3.2.2.2. 调参方法：网格搜索 [<sup>目录</sup>](#content)</h5></a>

对于SVM的RBF核，我们主要的调参方法都是交叉验证。具体在scikit-learn中，主要是使用网格搜索，即GridSearchCV类

```
from sklearn.model_selection import GridSearchCV
grid = GridSearchCV(SVC(), param_grid={"C":[0.1, 1, 10], "gamma": [1, 0.1, 0.01]}, cv=4)
grid.fit(X, y)
```

将GridSearchCV类用于SVM RBF调参时要注意的参数有：

> 1) **estimator**：即我们的模型，此处我们就是带高斯核的SVC或者SVR
> 
> 2) **param_grid**：即我们要调参的参数列表。 比如我们用SVC分类模型的话，那么param_grid可以定义为{"C":[0.1, 1, 10], "gamma": [0.1, 0.2, 0.3]}，这样我们就会有9种超参数的组合来进行网格搜索，选择一个拟合分数最好的超平面系数
> 
> 3) **cv**：S折交叉验证的折数，即将训练集分成多少份来进行交叉验证。默认是3。如果样本较多的话，可以适度增大cv的值

<a name="sklearn-svm-code"><h3>3.3. 编程实现 [<sup>目录</sup>](#content)</h3></a>

1. 生成测试数据

	```
	from sklearn.datasets import make_circles
	from sklearn.preprocessing import StandardScaler
	
	# 生成一些随机数据用于后续分类
	X, y = make_circles(noise=0.2, factor=0.5, random_state=1) # 生成时加入了一些噪声
	X = StandardScaler().fit_transform(X) # 把数据归一化
	
	```
	
	生成的随机数据可视化结果如下：
	
	<p align="center"><img src=./picture/InAction-MachineLearning-XGS-sklearn-SVM-5.png width=600 /></p>

2. 调参

	接着采用网格搜索的策略进行RBF核函数参数搜索
	
	```
	from sklearn.model_selection import GridSearchCV
	grid = GridSearchCV(SVC(), param_grid={"C":[0.1, 1, 10], "gamma": [1, 0.1, 0.01]}, cv=4) # 总共有9种参数组合的搜索空间
	grid.fit(X, y)
	print("The best parameters are %s with a score of %0.2f"
	      % (grid.best_params_, grid.best_score_))
	
	输出为：
	The best parameters are {'C': 10, 'gamma': 0.1} with a score of 0.91
	```

	可以对9种参数组合训练的结果进行可视化，观察分类的效果：

	```
	x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
	y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
	xx, yy = np.meshgrid(np.arange(x_min, x_max,0.02),
	                     np.arange(y_min, y_max, 0.02))
	
	for i, C in enumerate((0.1, 1, 10)):
	    for j, gamma in enumerate((1, 0.1, 0.01)):
	        plt.subplot()       
	        clf = SVC(C=C, gamma=gamma)
	        clf.fit(X,y)
	        Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])
	
	        # Put the result into a color plot
	        Z = Z.reshape(xx.shape)
	        plt.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.8)
	
	        # Plot also the training points
	        plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm)
	
	        plt.xlim(xx.min(), xx.max())
	        plt.ylim(yy.min(), yy.max())
	        plt.xticks(())
	        plt.yticks(())
	        plt.xlabel(" gamma=" + str(gamma) + " C=" + str(C))
	        plt.show()
	```

| `C \ gamma` | 1 | 0.1 | 0.001 |
|:---:|:---:|:---:|:---:|
| 0.1 | <img src=./picture/InAction-MachineLearning-XGS-sklearn-SVM-6.png width=300 />| <img src=./picture/InAction-MachineLearning-XGS-sklearn-SVM-7.png width=300 /> | <img src=./picture/InAction-MachineLearning-XGS-sklearn-SVM-8.png width=300 /> |
| 1 | <img src=./picture/InAction-MachineLearning-XGS-sklearn-SVM-9.png width=300 /> | <img src=./picture/InAction-MachineLearning-XGS-sklearn-SVM-10.png width=300 /> | <img src=./picture/InAction-MachineLearning-XGS-sklearn-SVM-11.png width=300 /> |
| 10 | <img src=./picture/InAction-MachineLearning-XGS-sklearn-SVM-12.png width=300 /> | <img src=./picture/InAction-MachineLearning-XGS-sklearn-SVM-13.png width=300 /> | <img src=./picture/InAction-MachineLearning-XGS-sklearn-SVM-14.png width=300 /> |

<a name="sklearn-naive-bayes"><h2>4.sklearn包中朴素贝叶斯库的使用 [<sup>目录</sup>](#content)</h2></a>

<a name="naive-bayes-basic-knowledge"><h3>4.1. 朴素贝叶斯相关知识点回顾 [<sup>目录</sup>](#content)</h3></a>

<a name="what-is-naive-bayes"><h4>4.1.1. 什么是朴素贝叶斯分类器 [<sup>目录</sup>](#content)</h4></a>

判别式模型（discriminative models)：像决策树、BP神经网络、支持向量机等，都可以归入判别式模型，它们都是直接学习出输出Y与特征X的关系，如：

- 决策函数 Y=f(X)
- 条件概率 P(Y|X)

生成式模型 (gernerative models)：先对联合概率分布 P(X,Y) 进行建模，然后再由此获得P(Y|X) = P(X,Y)/P(X)

贝叶斯学派的思想：

> 贝叶斯学派的思想可以概括为先验概率+数据=后验概率。也就是说我们在实际问题中需要得到的后验概率，可以通过先验概率和数据一起综合得到。数据大家好理解，被频率学派攻击的是先验概率，一般来说先验概率就是我们对于数据所在领域的历史经验，但是这个经验常常难以量化或者模型化，于是贝叶斯学派大胆的假设先验分布的模型，比如正态分布，beta分布等。这个假设一般没有特定的依据，因此一直被频率学派认为很荒谬。虽然难以从严密的数学逻辑里推出贝叶斯学派的逻辑，但是在很多实际应用中，贝叶斯理论很好用，比如垃圾邮件分类，文本分类

如何对P(X,Y)进行建模？

> 假如我们的分类模型样本是：
> 
> (x<sup>(1)</sup><sub>1</sub>,x<sup>(1)</sup><sub>2</sub>,...x<sup>(1)</sup><sub>n</sub>,y<sub>1</sub>), (x<sup>(2)</sup><sub>1</sub>,x<sup>(2)</sup><sub>2</sub>,...x<sup>(2)</sup><sub>n</sub>,y<sub>2</sub>),...(x<sup>(m)</sup><sub>1</sub>,x<sup>(m)</sup><sub>2</sub>, ..., x<sup>(m)</sup><sub>n</sub>,y<sub>m</sub>)
> 
> 则
> 
> P(X, Y) = P(Y) \* P( X = (x<sub>1</sub>, x<sub>2</sub>, ..., x<sub>n</sub>) | Y ) 
> 
> 其中
> 
> P( X = (x<sub>1</sub>, x<sub>2</sub>, ..., x<sub>n</sub>) | Y )  =  P( x<sub>1</sub> | Y) \* P( x<sub>2</sub> | Y, x<sub>1</sub>) \* ... P( x<sub>n</sub> | Y, x<sub>1</sub>, ... , x<sub>n-1</sub>)

这是一个超级复杂的有n个维度的条件分布，很难求出

朴素贝叶斯模型在这里做了一个大胆的假设，即X的n个维度之间相互独立，这样就可以得出：

P( X = (x<sub>1</sub>, x<sub>2</sub>, ..., x<sub>n</sub>) | Y ) = P( x<sub>1</sub> | Y) \* P( x<sub>2</sub> | Y) \* ... * P( x<sub>n</sub> | Y) 

<a name="naive-bayes-inference"><h4>4.1.2. 朴素贝叶斯推断 [<sup>目录</sup>](#content)</h4></a>

<p align="center"><img src=./picture/InAction-MachineLearning-XGS-sklearn-naive-bayes-1.png width=500 /></p>

<a name="naive-bayes-learning"><h4>4.1.3. 朴素贝叶斯学习 [<sup>目录</sup>](#content)</h4></a>

需要从训练样本中学习到以下两个参数：

- **先验概率** P(c)

	P(c)表示了样本空间中各类样本所占的比例
	
	根据大数定律，当训练集中包含充足的独立同分布样本时，P(c)可根据各类样本出现的频率来估计

	<p align="center"><img src=./picture/InAction-MachineLearning-XGS-sklearn-naive-bayes-2.png height=100 /></p>

- **类条件概率**（又称为似然） P(x<sub>i</sub> | c)

	（1）如果 x<sub>i</sub> 是离散的，可以假设 x<sub>i</sub>符合多项式分布，这样得到 P(x<sub>i</sub> | c) 是在样本类别 c 中，特征 x<sub>i</sub> 出现的频率
	
	<p align="center"><img src=./picture/InAction-MachineLearning-XGS-sklearn-naive-bayes-3.png height=100 /></p>

	（2）如果 x<sub>i</sub> 是连续属性，可以假设 P(x<sub>i</sub> | c) ~ N( μ<sub>c,i</sub> , σ<sup>2</sup><sub>c,i</sub> )
	
	<p align="center"><img src=./picture/InAction-MachineLearning-XGS-sklearn-naive-bayes-4.png height=100 /></p>

<a name="sklearn-naive-bayes-introduction"><h3>4.2. sklearn中朴素贝叶斯类库的简介 [<sup>目录</sup>](#content)</h3></a>

在scikit-learn中，一共有3个朴素贝叶斯的分类算法类

> - GaussianNB：先验为高斯分布的朴素贝叶斯
> 
> - MultinomialNB：先验为多项式分布的朴素贝叶斯
> 
> - BernoulliNB：先验为伯努利分布的朴素贝叶斯

这三个类适用的分类场景各不相同

> 一般来说，如果样本特征的分布大部分是**连续值**，使用GaussianNB会比较好
> 
> 如果如果样本特征的分大部分是**多元离散值**，使用MultinomialNB比较合适
> 
> 如果样本特征是**二元离散值**或者**很稀疏的多元离散值**，应该使用BernoulliNB。

<a name="aussian-nb"><h4>4.2.1. GaussianNB类 [<sup>目录</sup>](#content)</h4></a>

GaussianNB类的主要参数仅有一个，即先验概率priors

这个值默认不给出，如果不给出此时P(Y=c)= m<sub>c</sub> / m，如果给出的话就以priors 为准

在使用GaussianNB的fit方法拟合数据后，我们可以进行预测。此时预测有三种方法

> - predict方法：就是我们最常用的预测方法，直接给出测试集的预测类别输出；
> 
> - predict_proba方法：给出测试集样本在各个类别上预测的概率；
> 
> - predict_log_proba方法：和predict_proba类似，它会给出测试集样本在各个类别上预测的概率的一个对数转化；

```
from sklearn.naive_bayes import GaussianNB

clf = GaussianNB()
#拟合数据
clf.fit(X, Y)

#进行预测
clf.predict([[-0.8, -1]])
```

<a name="multinomial-nb"><h4>4.2.2. MultinomialNB类 [<sup>目录</sup>](#content)</h4></a>

MultinomialNB假设特征的先验概率为多项式分布，即如下式：

<p align="center"><img src=./picture/InAction-MachineLearning-XGS-sklearn-naive-bayes-5.png height=70 /></p>

MultinomialNB参数比GaussianNB多，但是一共也只有仅仅3个

- **参数alpha**：为上面的常数λ。如果你没有特别的需要，用默认的1即可。如果发现拟合的不好，需要调优时，可以选择稍大于1或者稍小于1的数

- **参数fit_prior**：是否要考虑先验概率，如果是false，则所有的样本类别输出都有相同的类别先验概率；否则可以自己用第三个参数class_prior输入先验概率，或者不输入第三个参数class_prior让MultinomialNB自己从训练集样本来计算先验概率

- **参数class_prior**：输入先验概率，若不输入第三个参数class_prior让MultinomialNB自己从训练集样本来计算先验概率

<a name="bernoulli-nb"><h4>4.2.3. BernoulliNB类 [<sup>目录</sup>](#content)</h4></a>

<p align="center"><img src=./picture/InAction-MachineLearning-XGS-sklearn-naive-bayes-6.png width=600 /></p>

其中，x <sub>i</sub> 只能取0或1

BernoulliNB一共有4个参数，其中3个参数的名字和意义和MultinomialNB完全相同

唯一增加的一个参数是binarize，这个参数主要是用来帮BernoulliNB处理二项分布的。如果不输入，则BernoulliNB认为每个数据特征都已经是二元的。否则的话，小于binarize的会归为一类，大于binarize的会归为另外一类

---

参考资料：

(1) [【GitHub】MLjian/TextClassificationImplement](https://github.com/MLjian/TextClassificationImplement)

(2) [刘建平Pinard《scikit-learn 逻辑回归类库使用小结》](https://www.cnblogs.com/pinard/p/6035872.html)

(3) [机器学习sklearn19.0——Logistic回归算法](https://blog.csdn.net/loveliuzz/article/details/78708359)

(4) [刘建平Pinard《scikit-learn决策树算法类库使用小结》](https://www.cnblogs.com/pinard/p/6056319.html)

(6) [刘建平Pinard《scikit-learn 支持向量机算法库使用小结》](https://www.cnblogs.com/pinard/p/6117515.html)

(7) [刘建平Pinard《支持向量机高斯核调参小结》](https://www.cnblogs.com/pinard/p/6126077.html)

(8) [loveliuzz《机器学习sklearn19.0——SVM算法》](https://blog.csdn.net/loveliuzz/article/details/78768063)

(9) 周志华《机器学习》

(10) [刘建平Pinard《朴素贝叶斯算法原理小结》](https://www.cnblogs.com/pinard/p/6069267.html)

(11) [刘建平Pinard《scikit-learn 朴素贝叶斯类库使用小结》](https://www.cnblogs.com/pinard/p/6074222.html)
